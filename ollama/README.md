# SkDucky AI - Ollama Private Service

This directory contains the configuration for running Ollama as a private service on Render.com.

## ğŸ—ï¸ Architecture

- **Main App**: SkDucky AI FastAPI application
- **Ollama Service**: Private service running CodeLlama model
- **Communication**: Internal networking between services

## ğŸ“ Files

- `Dockerfile`: Multi-stage build for Ollama with CodeLlama
- `entrypoint.sh`: Startup script that downloads CodeLlama model
- `README.md`: This documentation

## ğŸš€ Deployment

The private service is automatically deployed when you push to main branch.

1. Render builds the Ollama Docker image
2. Downloads CodeLlama model on startup
3. Main app connects via internal URL
4. CodeLlama + Examples hybrid system activated!

## ğŸ”§ Configuration

The main app receives the Ollama URL via environment variable:
```yaml
envVars:
  - key: OLLAMA_URL
    fromService:
      name: skducky-ollama
      type: pserv
      property: hostport
```

## ğŸ¦† Benefits

- âœ… Dedicated resources for Ollama
- âœ… CodeLlama model always available  
- âœ… Internal networking (secure + fast)
- âœ… Automatic scaling and management
- âœ… Perfect for hybrid AI system
